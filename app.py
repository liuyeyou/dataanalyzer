import streamlit as st
import matplotlib
import pandas as pd
import os

# Set the backend to a non-interactive one BEFORE importing pyplot
matplotlib.use("Agg")

from pandasai import SmartDataframe
from pandasai.core.response.chart import ChartResponse
from pandasai.core.response.dataframe import DataFrameResponse
from pandasai.core.prompts.base import BasePrompt

from src.ui import setup_page, setup_sidebar, display_chat_history, get_response_message, render_message
from src.data_processing import load_and_process_data
from src.llm_config import configure_llm
from src.agent_handler import create_extraction_agent, chat_with_agent
from src.intent_detector import get_intents
from src.prompts import ANALYSIS_PROMPT_TEMPLATE

@st.cache_resource
def get_llm(llm_option):
    """Cached function to configure and return an LLM."""
    print(f"--- [CACHE MISS] Configuring LLM: {llm_option} ---")
    return configure_llm(llm_option)

def main():
    """Main function to run the Streamlit application."""
    setup_page()
    llm_option, uploaded_file = setup_sidebar()
    st.title("LLM æ™ºèƒ½æ•°æ®åˆ†æåŠ©æ‰‹")

    data_container = st.container()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    display_chat_history()

    df = None
    if uploaded_file:
        df = load_and_process_data(uploaded_file, data_container)
    else:
        with data_container:
            pass
        st.info("è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼ ä¸€ä¸ª CSV æ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚")

    if question := st.chat_input("è¾“å…¥ä½ å…³äºæ•°æ®çš„é—®é¢˜..."):
        if df is None:
            st.warning("è¯·å…ˆä¸Šä¼ æ–‡ä»¶å†æé—®ã€‚")
            st.stop()
        
        user_message = {"role": "user", "type": "text", "content": question}
        st.session_state.messages.append(user_message)
        with st.chat_message("user"):
            render_message(user_message)

        try:
            llm = get_llm(llm_option)
            debug_container = st.expander("ğŸ› æ“ä½œæ—¥å¿—")

            with debug_container:
                st.write("Step 1: è¯†åˆ«ç”¨æˆ·æ„å›¾...")
            with st.spinner("æ­£åœ¨è¯†åˆ«æ‚¨çš„æ„å›¾..."):
                intents = get_intents(question)
            st.info(f"ğŸ¤– å·²è¯†åˆ«æ„å›¾: {', '.join(intents)}")
            
            # --- Final Pipeline ---
            
            # 1. PandasAI Agent for Data Extraction
            with debug_container:
                st.write("Step 2: ä½¿ç”¨ PandasAI Agent æå–åˆ†ææ‰€éœ€æ•°æ®...")
            with st.spinner("æ­£åœ¨æå–ç›¸å…³æ•°æ®..."):
                extraction_agent = create_extraction_agent(df, llm)
                extracted_data_response = chat_with_agent(extraction_agent, question)

            # --- Response Handling Branch ---
            # Handle DataFrame for text analysis using isinstance
            if isinstance(extracted_data_response, DataFrameResponse):
                extracted_df = extracted_data_response.value
                df_message = get_response_message(extracted_data_response, "dataframe")
                st.session_state.messages.append(df_message)
                with st.chat_message("assistant"):
                    render_message(df_message)

                # Continue to Step 3 for generating the text report
                if "string" in intents:
                    with debug_container:
                        st.write("Step 3: ç›´æ¥è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š...")
                    
                    data_csv = extracted_df.to_csv(index=False)
                    
                    final_prompt_str = ANALYSIS_PROMPT_TEMPLATE.format(
                        query=question,
                        data=data_csv
                    )
                    
                    prompt_obj = BasePrompt()
                    prompt_obj._resolved_prompt = final_prompt_str
                    
                    with st.spinner("æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š..."):
                        analysis_report = llm.call(prompt_obj)
                    
                    text_message = get_response_message(analysis_report, "string")
                    st.session_state.messages.append(text_message)
                    with st.chat_message("assistant"):
                        render_message(text_message)

            # Handle Plot for chart display using isinstance
            elif isinstance(extracted_data_response, ChartResponse):
                with debug_container:
                    st.write("Step 3: æ¥æ”¶åˆ°å›¾è¡¨å“åº”ï¼Œç›´æ¥æ˜¾ç¤ºå›¾è¡¨ã€‚")
                
                chart_path = extracted_data_response.value
                plot_message = get_response_message(chart_path, "plot")
                st.session_state.messages.append(plot_message)
                with st.chat_message("assistant"):
                    render_message(plot_message)

            # Handle any other unexpected response
            else:
                st.error("æ•°æ®æå–å¤±è´¥æˆ–è¿”å›äº†ä¸æ”¯æŒçš„å“åº”ç±»å‹ã€‚")
                st.exception(extracted_data_response)
                st.stop()

        except Exception as e:
            st.error("AI åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼æˆ–é—®é¢˜å†…å®¹ã€‚")
            st.exception(e)

if __name__ == "__main__":
    main()