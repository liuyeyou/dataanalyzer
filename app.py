import streamlit as st
import pandas as pd
import pandasai as pai
import os
import matplotlib
import re

# Set the backend to a non-interactive one BEFORE importing pyplot
matplotlib.use('AGG')

from pandasai_openai import OpenAI, AzureOpenAI
from pandasai_litellm import LiteLLM
from pandasai.core.response.dataframe import DataFrameResponse

# è®¾ç½® pandas æ˜¾ç¤ºæ‰€æœ‰åˆ—ï¼Œé˜²æ­¢æ•°æ®è¢«æˆªæ–­
pd.set_option('display.max_columns', None)

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="LLMæ™ºèƒ½æ•°æ®åˆ†æåŠ©æ‰‹", 
    layout="wide",
    initial_sidebar_state="expanded",
)

# --- æ³¨å…¥è‡ªå®šä¹‰ CSS ---
hide_streamlit_style = """
<style>
/* éšè—ä¸»èœå•å’Œé¡µè„š */
#MainMenu {visibility: hidden;}
footer {visibility: hidden;}
.stDeployButton {display: none;}

/* éšè— Streamlit å¤´éƒ¨ç™½æ¡ */
[data-testid="stHeader"] {
    display: none;
}
</style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.subheader("è®¾ç½®")
    llm_option = st.selectbox("é€‰æ‹©è¯­è¨€æ¨¡å‹", ["GPT-4o", "Gemini", "Deepseek"], index=0)
    st.divider()
    st.subheader("åŠ è½½æ•°æ®")
    uploaded_file = st.file_uploader("é€‰æ‹© CSV æ•°æ®æ–‡ä»¶", type="csv")

# --- ä¸»ä½“å†…å®¹ ---
st.title("LLM æ™ºèƒ½æ•°æ®åˆ†æåŠ©æ‰‹")


# åˆ›å»ºä¸€ä¸ªå®¹å™¨æ¥æ˜¾ç¤ºæ•°æ®æ ·æœ¬ï¼Œç¡®ä¿å®ƒæ€»æ˜¯åœ¨é¡¶éƒ¨
data_container = st.container()

# åˆå§‹åŒ–æˆ–æ˜¾ç¤ºèŠå¤©è®°å½•
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        if message.get("type") == "image":
            st.image(message["content"], caption="å†å²å›¾è¡¨")
        elif message.get("type") == "table":
            st.dataframe(pd.DataFrame(message["content"]))
        else:
            st.markdown(message["content"])

if uploaded_file is not None:
    # ä½¿ç”¨ pandas è¯»å– CSV
    df = pd.read_csv(uploaded_file)

    # æ¸…ç†åˆ—åï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦
    def clean_column_names(df):
        new_columns = {}
        for col in df.columns:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»é™¤æ‰€æœ‰éå­—æ¯ã€éæ•°å­—ã€éä¸­æ–‡å­—ç¬¦ï¼Œä¸‹åˆ’çº¿é™¤å¤–
            new_col = re.sub(r'[^\w\u4e00-\u9fa5]', '', str(col))
            new_columns[col] = new_col
        df.rename(columns=new_columns, inplace=True)
        return df

    df = clean_column_names(df)
    
    # åœ¨é¡¶éƒ¨çš„å®¹å™¨ä¸­æ˜¾ç¤ºæ•°æ®ä¿¡æ¯
    with data_container:
        st.success(f'æ•°æ®æ–‡ä»¶ "{uploaded_file.name}" åŠ è½½æˆåŠŸã€‚')
        st.write("æ•°æ®æ ·æœ¬:", df.head())
        st.divider()

    # LLMé…ç½®
    if llm_option == "GPT-4o":
        api_key = os.getenv("AZURE_OPENAI_KEY")
        api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
        if not api_key or not api_base:
            st.error("è¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® AZURE_OPENAI_KEY å’Œ AZURE_OPENAI_ENDPOINT")
            st.stop()
        llm = AzureOpenAI(
            deployment_name="deploy_gpt4o", # å‡è®¾æ‚¨çš„ Azure éƒ¨ç½²åç§°ä¸º deploy_gpt4o
            azure_endpoint=api_base,
            api_token=api_key,
            api_version="2024-02-01" # ä½¿ç”¨ä¸€ä¸ªè¾ƒæ–°çš„ç¨³å®š API ç‰ˆæœ¬
        )
    elif llm_option == "Gemini":
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            st.error("è¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® GOOGLE_API_KEY")
            st.stop()
        os.environ["GEMINI_API_KEY"] = api_key # LiteLLM aoturead
        llm = LiteLLM(model="gemini/gemini-1.5-pro-latest")
    elif llm_option == "Deepseek":
        api_key = os.getenv("DEEPSEEK_API_KEY")
        api_base = os.getenv("DEEPSEEK_API_URL")
        if not api_key or not api_base:
            st.error("è¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® DEEPSEEK_API_KEY å’Œ DEEPSEEK_API_URL")
            st.stop()
        llm = OpenAI(api_token=api_key, api_base=api_base, model="deepseek-chat", is_chat_model=True)
    else:
        st.error("æœªçŸ¥æ¨¡å‹ç±»å‹")
        st.stop()
        
    # æé—®
    question = st.chat_input("è¾“å…¥ä½ å…³äºæ•°æ®çš„é—®é¢˜...")
    if question:
        # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
        with st.chat_message("user"):
            st.markdown(question)
        # è®°å½•ç”¨æˆ·é—®é¢˜
        st.session_state.messages.append({"role": "user", "content": question})

        with st.spinner("AI æ­£åœ¨æ€è€ƒä¸­..."):
            try:
                # ä½¿ç”¨ v3 Agent, å…ˆå°† pandas.DataFrame åŒ…è£…æˆ pandasai.DataFrame
                pai_df = pai.DataFrame(df)

                # å®šä¹‰ç³»ç»Ÿ Prompt
                system_prompt = """ä½ ç°åœ¨æ˜¯ä¸€åä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œè¯·ä¸¥æ ¼ä¾æ®ç”¨æˆ·æä¾›çš„ DataFrame ä½œç­”ã€‚

**å›ç­”è¦æ±‚ï¼š**
1.  **æ•°æ®é©±åŠ¨ï¼š** æ‰€æœ‰åˆ†æå’Œç»“è®ºå¿…é¡»åŸºäºè¡¨æ ¼ä¸­çš„æ•°æ®ï¼Œæ˜ç¡®åˆ—å‡ºå¼•ç”¨çš„æ•°æ®åˆ—å’Œæ•°å€¼ã€‚
2.  **æ·±åº¦åˆ†æï¼š** è¿›è¡Œå¿…è¦çš„ç»Ÿè®¡è®¡ç®—ï¼ˆå¦‚å‡å€¼ã€å¢é•¿ç‡ã€å æ¯”ï¼‰ï¼Œå¹¶è§£é‡Šè®¡ç®—è¿‡ç¨‹å’Œç»“æœçš„ä¸šåŠ¡å«ä¹‰ã€‚
3.  **é€»è¾‘ä¸¥è°¨ï¼š** ç»“è®ºéœ€æœ‰æ¸…æ™°çš„é€»è¾‘æ”¯æ’‘ï¼Œé¿å…ç©ºæ³›æˆ–æ¨æµ‹æ€§çš„æè¿°ã€‚
4.  **ç»“æ„åŒ–è¾“å‡ºï¼š** è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ Markdown æ ¼å¼è¿›è¡Œå›ç­”ï¼Œæ¯ä¸ªè¦ç‚¹ç‹¬å ä¸€è¡Œï¼Œä¸è¦ä½¿ç”¨ JSON æˆ–å­—å…¸æ ¼å¼ã€‚
5.  **ä¸­æ–‡å›ç­”ï¼š** å…¨éƒ¨ä½¿ç”¨ä¸­æ–‡ã€‚

**è¾“å‡ºæ ¼å¼ï¼š**
```markdown
ğŸ“Šã€æ•°æ®ç®€è¿°ã€‘
- **æ ¸å¿ƒå­—æ®µ:** [ç”¨äºåˆ†æçš„å…³é”®å­—æ®µåˆ—è¡¨]
- **æ•°æ®èŒƒå›´:** [åˆ†ææ‰€æ¶‰åŠçš„æ—¶é—´æˆ–å…¶ä»–ç»´åº¦èŒƒå›´]
- **æ ·æœ¬é‡:** [æ‰€åˆ†æçš„æ•°æ®è¡Œæ•°]

ğŸ“‰ã€æ•°å€¼åˆ†æã€‘
- **[æŒ‡æ ‡]å˜åŒ–:** [å…·ä½“æ•°å€¼å¯¹æ¯”], å·®å€¼: [å·®å€¼], å˜åŒ–ç‡: [å˜åŒ–ç‡]
- **[æ”¶å…¥æ„æˆ]åˆ†æ:** [å„éƒ¨åˆ†æ”¶å…¥çš„å æ¯”åŠå˜åŒ–æƒ…å†µ]

ğŸ”ã€è¶‹åŠ¿ä¸å¼‚å¸¸ã€‘
- **è¶‹åŠ¿å˜åŒ–:** [ä¸Šå‡/ä¸‹é™/å¹³ç¨³]ï¼Œå¹¶ç»“åˆæ•°æ®è¯´æ˜ã€‚
- **å¼‚å¸¸ç‚¹:** [æ˜¯å¦å­˜åœ¨å¼‚å¸¸æ•°æ®ç‚¹ï¼Œå¦‚æœ‰è¯·æŒ‡å‡ºå¹¶åˆ†æ]ã€‚

ğŸ“Œã€åŸå› æ¨æµ‹ã€‘
- **ä¸»è¦åŸå› :** [åŸºäºæ•°æ®åˆ†æï¼Œæ¨æ–­å¯¼è‡´å˜åŒ–çš„æ ¸å¿ƒåŸå› ï¼Œä¾‹å¦‚ï¼šXXæ”¶å…¥ä¸‹é™æ˜¯æ€»æ”¶å…¥é™ä½çš„ä¸»è¦åŸå› ]ã€‚
- **æ¬¡è¦åŸå› :** [å…¶ä»–å½±å“å› ç´ ]ã€‚

ğŸ“¢ã€å»ºè®®ä¸è¡ŒåŠ¨ã€‘
- **ä¸šåŠ¡å»ºè®®:** [é’ˆå¯¹åˆ†æç»“æœï¼Œæå‡ºå…·ä½“çš„ä¸šåŠ¡æ“ä½œå»ºè®®]ã€‚
- **æ”¹è¿›æ–¹å‘:** [å¯ä»¥ä»å“ªäº›æ–¹é¢ç€æ‰‹æ”¹è¿›]ã€‚
- **åç»­éªŒè¯:** [å¦‚ä½•è·Ÿè¸ªå’ŒéªŒè¯å»ºè®®æªæ–½çš„æ•ˆæœ]ã€‚
```"""
                save_path = os.path.join(os.getcwd(), "exports/charts")
                os.makedirs(save_path, exist_ok=True)  # è‡ªåŠ¨åˆ›å»ºç›®å½•ï¼ˆå¦‚ä¸å­˜åœ¨ï¼‰
                # åˆå§‹åŒ– PandasAI Agent
                agent = pai.Agent(pai_df, config={
                    "llm": llm,
                    "enable_cache": False,
                    "verbose": True,
                    "conversational": False,
                    "use_sql": False, # ç¦ç”¨ SQL æŸ¥è¯¢ä»¥é¿å…å†…éƒ¨ bug
                    "custom_whitelisted_dependencies": ["pandas"],
                    "prompt": system_prompt,
                     "save_charts_path": save_path,  # âœ… æ–°å¢ï¼šå›¾è¡¨è¾“å‡ºç›®å½•
                })

                # è·å–å›ç­”
                answer = agent.chat(question)

                # æ˜¾ç¤ºAIå›ç­”
                with st.chat_message("assistant"):
                    # 1. å›¾è¡¨å±•ç¤ºï¼ˆæœ¬åœ° PNG è·¯å¾„ï¼‰
                    if isinstance(answer, str) and answer.endswith('.png') and os.path.exists(answer):
                        # å¦‚æœæ˜¯ PandasAI ç”Ÿæˆçš„å›¾è¡¨è·¯å¾„ï¼Œæ˜¾ç¤ºå›¾ç‰‡
                        st.image(answer, caption="AI ç”Ÿæˆçš„å›¾è¡¨")
                        # ä¿å­˜æ¶ˆæ¯ç”¨äºå›æ˜¾ï¼Œä¸ç”¨å†™æˆ markdown
                        st.session_state.messages.append({
                            "role": "assistant",
                            "type": "image",
                            "content": answer
                        })

                    # 2. è¡¨æ ¼ç»“æœï¼ˆDataFrameResponse ç±»å‹ï¼‰
                    elif isinstance(answer, DataFrameResponse):
                        df_to_display = answer.value
                        st.dataframe(df_to_display)
                        st.session_state.messages.append({
                            "role": "assistant",
                            "type": "table",
                            "content": df_to_display.to_dict()
                        })

                    # 3. æ™®é€šå­—ç¬¦ä¸²æ–‡æœ¬ï¼ˆåˆ†æç»“æœã€è‡ªç„¶è¯­è¨€è§£é‡Šç­‰ï¼‰
                    elif isinstance(answer, str):
                        st.markdown(answer)
                        st.session_state.messages.append({
                            "role": "assistant",
                            "type": "text",
                            "content": answer
                        })

                    # 4. å…œåº•å¤„ç†ï¼ˆæœªçŸ¥ç±»å‹ï¼‰
                    else:
                        st.markdown(str(answer))
                        st.session_state.messages.append({
                            "role": "assistant",
                            "type": "text",
                            "content": str(answer)
                        })


            except Exception as e:
                st.error("AI åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼æˆ–é—®é¢˜å†…å®¹ã€‚")
                st.exception(e)  # æ›´ç›´è§‚æ˜¾ç¤ºå®Œæ•´æŠ¥é”™ä¿¡æ¯
                st.stop()
else:
    st.info("è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼ ä¸€ä¸ª CSV æ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚")
