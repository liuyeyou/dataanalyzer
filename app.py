import streamlit as st
import matplotlib
import pandas as pd
import os

# Set the backend to a non-interactive one BEFORE importing pyplot
matplotlib.use("Agg")

from pandasai import SmartDataframe
from pandasai.core.response.chart import ChartResponse
from pandasai.core.response.dataframe import DataFrameResponse
from pandasai.core.prompts.base import BasePrompt

from src.ui import setup_page, setup_sidebar, display_chat_history, get_response_message, render_message
from src.data_processing import load_and_process_data
from src.llm_config import configure_llm
from src.agent_handler import create_extraction_agent, create_processing_agent, chat_with_agent
from src.intent_detector import get_intents, get_analysis_type
from src.prompts import (
    ANALYSIS_PROMPT_TEMPLATE, 
    GUIDANCE_PROMPT_TEMPLATE, 
    SIMPLIFICATION_PROMPT_TEMPLATE, 
    PLOT_REQUEST_EXTRACTION_PROMPT_TEMPLATE,
    SIMPLE_ANSWER_PROMPT_TEMPLATE,
    DATAFRAME_REQUEST_EXTRACTION_PROMPT_TEMPLATE
)

@st.cache_resource
def get_llm(llm_option):
    """Cached function to configure and return an LLM."""
    print(f"--- [CACHE MISS] Configuring LLM: {llm_option} ---")
    return configure_llm(llm_option)

def main():
    """Main function to run the Streamlit application."""
    setup_page()
    llm_option, uploaded_file = setup_sidebar()
    st.title("LLM æ™ºèƒ½æ•°æ®åˆ†æåŠ©æ‰‹")

    data_container = st.container()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    display_chat_history()

    df = None
    if uploaded_file:
        df = load_and_process_data(uploaded_file, data_container)
    else:
        with data_container:
            pass
        st.info("è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼ ä¸€ä¸ª CSV æ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚")

    if question := st.chat_input("è¾“å…¥ä½ å…³äºæ•°æ®çš„é—®é¢˜..."):
        if df is None:
            st.warning("è¯·å…ˆä¸Šä¼ æ–‡ä»¶å†æé—®ã€‚")
            st.stop()
        
        user_message = {"role": "user", "type": "text", "content": question}
        st.session_state.messages.append(user_message)
        with st.chat_message("user"):
            render_message(user_message)

        try:
            llm = get_llm(llm_option)
            debug_container = st.expander("ğŸ› æ“ä½œæ—¥å¿—")

            # --- Step 1: Intent & Analysis Type Detection ---
            with debug_container:
                st.write("Step 1: è¯†åˆ«ç”¨æˆ·æ„å›¾å’Œé—®é¢˜ç±»å‹...")
            with st.spinner("æ­£åœ¨è¯†åˆ«æ‚¨çš„æ„å›¾..."):
                intents = get_intents(question)
            st.info(f"ğŸ¤– å·²è¯†åˆ«æ„å›¾: {', '.join(intents)}")
            
            analysis_type = None
            if "string" in intents:
                with st.spinner("æ­£åœ¨åˆ¤æ–­é—®é¢˜ç±»å‹..."):
                    analysis_type = get_analysis_type(question)
                st.info(f"ğŸ§ é—®é¢˜ç±»å‹: {analysis_type}")

            # --- ROUTING ---
            
            # Route 1: Deep Analysis Pipeline (highest priority)
            if analysis_type == 'deep_analysis':
                with debug_container:
                    st.write("Step 2: æ£€æµ‹åˆ°æ·±åº¦åˆ†æéœ€æ±‚ï¼Œå¯åŠ¨å®Œæ•´åˆ†ææµç¨‹...")
                
                # --- 2a. Data Extraction ---
                with st.spinner("æ­£åœ¨ç”Ÿæˆæ•°æ®æå–æŒ‡ä»¤..."):
                    simplification_prompt_str = SIMPLIFICATION_PROMPT_TEMPLATE.format(question=question)
                    prompt_obj = BasePrompt()
                    prompt_obj._resolved_prompt = simplification_prompt_str
                    simplified_question = llm.call(prompt_obj)
                with debug_container:
                    st.subheader("Step 2a: ç®€åŒ–åçš„æå–é—®é¢˜")
                    st.write(simplified_question)

                with st.spinner("æ­£åœ¨æå–ç›¸å…³æ•°æ®..."):
                    extraction_agent = create_extraction_agent(df, llm)
                    extracted_data_response = chat_with_agent(extraction_agent, simplified_question)
                
                if not isinstance(extracted_data_response, DataFrameResponse) or extracted_data_response.value.empty:
                    st.error("æ·±åº¦åˆ†æçš„æ•°æ®æå–æ­¥éª¤æœªèƒ½è¿”å›æœ‰æ•ˆçš„è¡¨æ ¼æ•°æ®ã€‚")
                    st.stop()
                
                extracted_df = extracted_data_response.value
                df_message = get_response_message(extracted_data_response, "dataframe")
                st.session_state.messages.append(df_message)
                with st.chat_message("assistant"):
                    render_message(df_message)

                # --- 2b. Textual Analysis (already part of this flow) ---
                with debug_container:
                    st.write("Step 3: ç”ŸæˆåŠ¨æ€åˆ†ææŒ‡å¯¼...")
                with st.spinner("ç”Ÿæˆä¸“å±åˆ†ææŒ‡ä»¤..."):
                    data_sample_csv = extracted_df.head().to_csv(index=False)
                    prompt_for_guidance = GUIDANCE_PROMPT_TEMPLATE.format(question=question, data_sample=data_sample_csv)
                    guidance_prompt_obj = BasePrompt()
                    guidance_prompt_obj._resolved_prompt = prompt_for_guidance
                    analysis_guidance = llm.call(guidance_prompt_obj)
                
                with debug_container:
                    st.subheader("Step 3a: ç”Ÿæˆçš„åˆ†ææŒ‡å¯¼")
                    st.write(analysis_guidance)
                
                with st.spinner("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š..."):
                    data_csv = extracted_df.to_csv(index=False)
                    final_prompt_str = ANALYSIS_PROMPT_TEMPLATE.format(query=question, guidance=analysis_guidance, data=data_csv)
                    prompt_obj = BasePrompt()
                    prompt_obj._resolved_prompt = final_prompt_str
                    analysis_report = llm.call(prompt_obj)
                text_message = get_response_message(analysis_report, "string")
                st.session_state.messages.append(text_message)
                with st.chat_message("assistant"):
                    render_message(text_message)
                
                # --- 2c. Plotting (if also requested) ---
                if "plot" in intents:
                    with debug_container:
                        st.write("Step 4: æ£€æµ‹åˆ°ç»˜å›¾æ„å›¾ï¼Œåœ¨åˆ†æåŸºç¡€ä¸Šç”Ÿæˆå›¾è¡¨...")
                    with st.spinner("æ­£åœ¨æå–ç»˜å›¾æŒ‡ä»¤..."):
                        plot_request_prompt_str = PLOT_REQUEST_EXTRACTION_PROMPT_TEMPLATE.format(question=question)
                        prompt_obj = BasePrompt()
                        prompt_obj._resolved_prompt = plot_request_prompt_str
                        plot_question = llm.call(prompt_obj)
                    
                    with st.spinner("æ­£åœ¨ç”Ÿæˆå›¾è¡¨..."):
                        # Use the already extracted dataframe
                        chart_agent = create_processing_agent(extracted_df, llm)
                        response = chat_with_agent(chart_agent, plot_question)
                    
                    if isinstance(response, ChartResponse):
                        plot_message = get_response_message(response.value, "plot")
                        st.session_state.messages.append(plot_message)
                        with st.chat_message("assistant"):
                            render_message(plot_message)
                    else:
                        st.warning("å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼Œä»£ç†è¿”å›äº†éå›¾è¡¨å“åº”ã€‚")

            # Route 2: Direct Processing for all other cases
            else:
                with debug_container:
                    st.write("Step 2: æ£€æµ‹åˆ°ç›´æ¥å¤„ç†éœ€æ±‚ï¼Œå¼€å§‹æŒ‰åºå¤„ç†...")
                
                processing_agent = create_processing_agent(df, llm)
                
                # Sequentially handle plotting
                if "plot" in intents:
                    with debug_container:
                        st.write("  - å¤„ç†ç»˜å›¾è¯·æ±‚...")
                    with st.spinner("æ­£åœ¨æå–ç»˜å›¾æŒ‡ä»¤..."):
                        plot_request_prompt_str = PLOT_REQUEST_EXTRACTION_PROMPT_TEMPLATE.format(question=question)
                        prompt_obj = BasePrompt()
                        prompt_obj._resolved_prompt = plot_request_prompt_str
                        plot_question = llm.call(prompt_obj)
                    
                    if plot_question:
                        with st.spinner("æ­£åœ¨ç”Ÿæˆå›¾è¡¨..."):
                            response = chat_with_agent(processing_agent, plot_question)
                        if isinstance(response, ChartResponse):
                            plot_message = get_response_message(response.value, "plot")
                            st.session_state.messages.append(plot_message)
                            with st.chat_message("assistant"):
                                render_message(plot_message)
                        else:
                            st.warning("å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼Œä»£ç†è¿”å›äº†éå›¾è¡¨å“åº”ã€‚")

                # Sequentially handle dataframe calculation
                if "dataframe" in intents:
                    with debug_container:
                        st.write("  - å¤„ç†è¡¨æ ¼/è®¡ç®—è¯·æ±‚...")

                    # Use the prompt from the central file
                    with st.spinner("æ­£åœ¨æå–è¡¨æ ¼è®¡ç®—æŒ‡ä»¤..."):
                        dataframe_request_prompt_str = DATAFRAME_REQUEST_EXTRACTION_PROMPT_TEMPLATE.format(question=question)
                        prompt_obj = BasePrompt()
                        prompt_obj._resolved_prompt = dataframe_request_prompt_str
                        dataframe_question = llm.call(prompt_obj)

                    if dataframe_question:
                        with st.spinner("æ­£åœ¨è®¡ç®—å¹¶ç”Ÿæˆè¡¨æ ¼..."):
                            response = chat_with_agent(processing_agent, dataframe_question)
                        if isinstance(response, DataFrameResponse):
                            df_message = get_response_message(response, "dataframe")
                            st.session_state.messages.append(df_message)
                            with st.chat_message("assistant"):
                                render_message(df_message)
                        else:
                             # If it fails to return a dataframe, it might return a string answer
                             text_message = get_response_message(str(response), "string")
                             st.session_state.messages.append(text_message)
                             with st.chat_message("assistant"):
                                 render_message(text_message)

                # Handle simple lookups that are not plots or dataframes
                if "string" in intents and analysis_type == 'simple_lookup' and not ("plot" in intents or "dataframe" in intents):
                    with debug_container:
                        st.write("  - å¤„ç†ç®€å•é—®ç­”è¯·æ±‚...")
                    with st.spinner("æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ..."):
                        response = chat_with_agent(processing_agent, question)
                        text_message = get_response_message(str(response), "string")
                        st.session_state.messages.append(text_message)
                        with st.chat_message("assistant"):
                            render_message(text_message)

        except Exception as e:
            st.error("AI åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼æˆ–é—®é¢˜å†…å®¹ã€‚")
            st.exception(e)

if __name__ == "__main__":
    main()